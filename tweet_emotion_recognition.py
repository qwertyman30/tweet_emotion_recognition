# -*- coding: utf-8 -*-
"""tweet emotion recognition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18Hq1_OCImg15xZep9dxNLYvqIHG6Le2-
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import nlp
import random
from sklearn.metrics import confusion_matrix
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

def show_history(h):
    epochs = len(h.history['loss'])
    plt.figure(figsize=(16, 6))

    plt.subplot(1, 2, 2)
    plt.plot(range(1, epochs+1), h.history.get('accuracy'), label="Training")
    plt.plot(range(1, epochs+1), h.history.get('val_accuracy'), label="Validation")
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()
    
    plt.subplot(1, 2, 2)
    plt.plot(range(1, epochs+1), h.history.get('loss'), label="Training")
    plt.plot(range(1, epochs+1), h.history.get('val_loss'), label="Validation")
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

def show_confusion_matrix(y_true, y_pred, classes):
  cm = confusion_matrix(y_true, y_pred, normalize='true')
  
  plt.figure(figsize=(8, 8))
  sp = plt.subplot(1, 1, 1)
  ctx = sp.matshow(cm)
  plt.xticks(list(range(0, 6)), labels=classes)
  plt.yticks(list(range(0, 6)), labels=classes)
  plt.colorbar(ctx)
  plt.show()

dataset = nlp.load_dataset('emotion')

train = dataset['train']
val = dataset['validation']
test = dataset['test']

def get_data_labels(data):
  tweets = [x['text'] for x in data]
  labels = [x['label'] for x in data]
  return tweets, labels

x_train, y_train = get_data_labels(train)
x_val, y_val = get_data_labels(val)
x_test, y_test = get_data_labels(test)

tokenizer = Tokenizer(num_words=10000, oov_token="<UNK>")
tokenizer.fit_on_texts(x_train)

lengths = [len(t.split(' ')) for t in x_train]
plt.hist(lengths, bins=len(set(lengths)))
plt.show()

max_len = 50

def get_sequences(tokenizer, tweets):
  sequences = tokenizer.texts_to_sequences(tweets)
  padded_sequences = pad_sequences(sequences, maxlen=max_len, truncating='post', padding='post')
  return padded_sequences

padded_train_seq = get_sequences(tokenizer, x_train)

classes = set(y_train)

plt.hist(y_train, bins=11)
plt.show()

class_to_index = dict((c, i) for i, c in enumerate(classes))
index_to_class = dict((v, k) for k, v in class_to_index.items())

names_to_ids = lambda labels: np.array([class_to_index.get(x) for x in labels])

y_train = names_to_ids(y_train)

model = tf.keras.models.Sequential([
  tf.keras.layers.Embedding(10000, 16, input_length=max_len),
  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(20, activation='tanh', return_sequences='true')),
  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(20, activation='tanh')),
  tf.keras.layers.Dense(6, activation='softmax')
])

model.compile(
    loss='sparse_categorical_crossentropy',
    optimizer='adam',
    metrics=['accuracy']
)
print(model.summary())

x_val = get_sequences(tokenizer, x_val)
y_val = names_to_ids(y_val)

h = model.fit(padded_train_seq, y_train, validation_data=(x_val, y_val), epochs=25,
              callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=4)])

show_history(h)

x_test = get_sequences(tokenizer, x_test)
y_test = names_to_ids(y_test)

_ = model.evaluate(x_test, y_test)

for _ in range(10):
  x = random.randint(0, len(x_test)-1)
  print(f"Sentence: {test[x]['text']}")
  print(f'Emotion: {index_to_class[y_test[x]]}')
  prediction = model.predict(np.expand_dims(x_test[x], axis=0))[0]
  pred = index_to_class[np.argmax(prediction)]
  print(f"Prediction: {pred}")
  print()

preds = model.predict_classes(x_test)
show_confusion_matrix(y_test, preds, list(classes))

